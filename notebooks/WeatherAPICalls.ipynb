{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from geopy.distance import geodesic\n",
    "from meteostat import Stations, Daily\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from meteostat import Stations, Daily\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download weather station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from meteostat import Stations, Daily\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Point\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Load GADM region boundaries for Greece\n",
    "greece_regions = gpd.read_file(\n",
    "    \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Data/gadm41_GRC_1.json\"\n",
    ")\n",
    "\n",
    "# Fetch Greek weather stations\n",
    "stations = Stations().region(\"GR\").fetch()\n",
    "stations_df = stations.reset_index()[['id', 'latitude', 'longitude']]\n",
    "\n",
    "# Convert stations to GeoDataFrame\n",
    "stations_gdf = gpd.GeoDataFrame(\n",
    "    stations_df,\n",
    "    geometry=gpd.points_from_xy(stations_df['longitude'], stations_df['latitude']),\n",
    "    crs=greece_regions.crs\n",
    ")\n",
    "\n",
    "# Spatially join stations to regions\n",
    "stations_with_regions = gpd.sjoin(\n",
    "    stations_gdf,\n",
    "    greece_regions[['NAME_1', 'geometry']],\n",
    "    predicate='intersects',\n",
    "    how='inner'\n",
    ").rename(columns={'NAME_1': 'region'})\n",
    "\n",
    "# Define time period\n",
    "start = datetime(2000, 1, 1)\n",
    "end = datetime(2024, 12, 31)\n",
    "\n",
    "weather_list = []\n",
    "weather_columns = ['tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wdir', 'wspd', 'wpgt', 'pres', 'tsun']\n",
    "\n",
    "for station_id in stations_with_regions['id'].unique():\n",
    "    try:\n",
    "        weather = Daily(station_id, start, end).fetch()[weather_columns]\n",
    "        if not weather.empty:\n",
    "            weather['station_id'] = station_id\n",
    "            weather['date'] = weather.index\n",
    "            weather.reset_index(drop=True, inplace=True)\n",
    "            weather_list.append(weather)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching station {station_id}: {e}\")\n",
    "\n",
    "# Combine data\n",
    "weather_df = pd.concat(weather_list, ignore_index=True)\n",
    "\n",
    "# Merge with region data\n",
    "weather_with_regions = weather_df.merge(\n",
    "    stations_with_regions[['id', 'region']],\n",
    "    left_on='station_id',\n",
    "    right_on='id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Aggregate data by region and date\n",
    "region_weather = weather_with_regions.groupby(['region', 'date']).agg({\n",
    "    'tavg': 'mean',\n",
    "    'tmax': 'mean',\n",
    "    'tmin': 'mean',\n",
    "    'prcp': 'mean',\n",
    "    'snow': 'mean',\n",
    "    'wdir': 'mean',\n",
    "    'wspd': 'mean',\n",
    "    'wpgt': 'mean',\n",
    "    'pres': 'mean',\n",
    "    'tsun': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename to your preferred columns\n",
    "region_weather.rename(columns={\n",
    "    'tavg': 'temp_avg',\n",
    "    'tmax': 'temp_max',\n",
    "    'tmin': 'temp_min',\n",
    "    'prcp': 'precip',\n",
    "    'snow': 'snow_depth',\n",
    "    'wdir': 'wind_dir',\n",
    "    'wspd': 'wind_speed',\n",
    "    'wpgt': 'wind_gust',\n",
    "    'pres': 'pressure',\n",
    "    'tsun': 'sunshine_duration'\n",
    "}, inplace=True)\n",
    "\n",
    "# ----------- FILL MISSING REGION-DATE USING NEAREST REGION ----------- #\n",
    "\n",
    "# Generate full region-date index\n",
    "dates_full = pd.date_range(start=start, end=end)\n",
    "regions = region_weather['region'].unique()\n",
    "full_index = pd.MultiIndex.from_product([regions, dates_full], names=['region', 'date'])\n",
    "\n",
    "# Reindex to introduce missing rows\n",
    "region_weather_full = region_weather.set_index(['region', 'date']).reindex(full_index).reset_index()\n",
    "\n",
    "# Project to metric CRS for accurate centroids\n",
    "greece_regions['region'] = greece_regions['NAME_1']\n",
    "greece_projected = greece_regions.to_crs(epsg=3857)\n",
    "greece_regions['centroid'] = greece_projected['geometry'].centroid\n",
    "\n",
    "# Convert back to lat/lon for distance calcs\n",
    "greece_regions['centroid_coords'] = greece_regions['centroid'].to_crs(epsg=4326).apply(lambda g: (g.y, g.x))\n",
    "region_coords = greece_regions.set_index('region')['centroid_coords'].to_dict()\n",
    "\n",
    "# Fill missing rows using closest region with data on same date\n",
    "filled_rows = []\n",
    "for idx, row in region_weather_full[region_weather_full.isnull().any(axis=1)].iterrows():\n",
    "    target_region = row['region']\n",
    "    date = row['date']\n",
    "    target_coord = region_coords.get(target_region)\n",
    "    if not target_coord:\n",
    "        continue\n",
    "\n",
    "    available = region_weather_full[\n",
    "        (region_weather_full['date'] == date) &\n",
    "        (region_weather_full['region'] != target_region)\n",
    "    ].dropna()\n",
    "\n",
    "    if available.empty:\n",
    "        continue\n",
    "\n",
    "    available = available.copy()\n",
    "    available['dist'] = available['region'].apply(lambda r: geodesic(target_coord, region_coords[r]).km)\n",
    "    closest = available.sort_values('dist').iloc[0]\n",
    "    filled_row = row.copy()\n",
    "    filled_row.update(closest.drop(['region', 'date', 'dist']))\n",
    "    filled_rows.append(filled_row)\n",
    "\n",
    "# Merge back filled rows\n",
    "filled_df = pd.DataFrame(filled_rows)\n",
    "region_weather_final = pd.concat([\n",
    "    region_weather_full.dropna(),\n",
    "    filled_df\n",
    "], ignore_index=True).sort_values(['region', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Save final dataset\n",
    "region_weather_final.to_csv(\n",
    "    \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Data/region_weather_2000_2024.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(region_weather_final.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region\n",
       "Aegean                          9132\n",
       "Crete                           9132\n",
       "Peloponnese,WesternGreeceand    9132\n",
       "MacedoniaandThrace              9126\n",
       "Attica                          9117\n",
       "ThessalyandCentralGreece        8882\n",
       "EpirusandWesternMacedonia       4129\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_weather['region'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Data/region_weather_epirus_patched.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region\n",
       "Aegean                          9132\n",
       "Crete                           9132\n",
       "Peloponnese,WesternGreeceand    9132\n",
       "MacedoniaandThrace              9126\n",
       "Attica                          9117\n",
       "ThessalyandCentralGreece        8882\n",
       "EpirusandWesternMacedonia       8398\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['region'].value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Trying station: Ioannina Airport (16642)\n",
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Kastoria Airport (16614)\n",
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Kozani Airport (16632)\n",
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Korca (13629)\n",
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Gjirokastra (13625)\n",
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Florina (16613)\n",
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Pretor-Pgc (13580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Kerkyra Airport (16641)\n",
      "‚úÖ Data found: 4269 rows\n",
      "\n",
      "üöÄ Using patch from: Kerkyra Airport\n",
      "‚úÖ Final patched dataset saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from meteostat import Daily\n",
    "from datetime import datetime\n",
    "\n",
    "# List of fallback station IDs near Epirus\n",
    "fallback_stations = [\n",
    "    ('16642', 'Ioannina Airport'),\n",
    "    ('16614', 'Kastoria Airport'),\n",
    "    ('16632', 'Kozani Airport'),\n",
    "    ('13629', 'Korca'),\n",
    "    ('13625', 'Gjirokastra'),\n",
    "    ('16613', 'Florina'),\n",
    "    ('13580', 'Pretor-Pgc'),\n",
    "    ('16641', 'Kerkyra Airport'),\n",
    "    ('16648', 'Larissa Airport'),\n",
    "    ('13583', 'Bitola')\n",
    "]\n",
    "\n",
    "# Define time range\n",
    "start = datetime(2000, 1, 1)\n",
    "end = datetime(2011, 9, 8)\n",
    "\n",
    "# Placeholder for best patch\n",
    "best_patch = None\n",
    "best_station = None\n",
    "\n",
    "# Try each fallback station\n",
    "for station_id, station_name in fallback_stations:\n",
    "    print(f\"\\nüîç Trying station: {station_name} ({station_id})\")\n",
    "    try:\n",
    "        patch_df = Daily(station_id, start, end).fetch()\n",
    "\n",
    "        if not patch_df.empty:\n",
    "            print(f\"‚úÖ Data found: {patch_df.shape[0]} rows\")\n",
    "\n",
    "            patch_df['date'] = patch_df.index\n",
    "            patch_df['region'] = 'EpirusandWesternMacedonia'\n",
    "            patch_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            patch_df.rename(columns={\n",
    "                'tavg': 'temp_avg',\n",
    "                'tmax': 'temp_max',\n",
    "                'tmin': 'temp_min',\n",
    "                'prcp': 'precip',\n",
    "                'snow': 'snow_depth',\n",
    "                'wdir': 'wind_dir',\n",
    "                'wspd': 'wind_speed',\n",
    "                'wpgt': 'wind_gust',\n",
    "                'pres': 'pressure',\n",
    "                'tsun': 'sunshine_duration'\n",
    "            }, inplace=True)\n",
    "\n",
    "            patch_columns = ['region', 'date', 'temp_avg', 'temp_max', 'temp_min',\n",
    "                             'precip', 'snow_depth', 'wind_dir', 'wind_speed',\n",
    "                             'wind_gust', 'pressure', 'sunshine_duration']\n",
    "\n",
    "            patch_df = patch_df[patch_columns]\n",
    "\n",
    "            best_patch = patch_df\n",
    "            best_station = station_name\n",
    "            break  # Stop at first with data\n",
    "\n",
    "        else:\n",
    "            print(\"‚ùå No data available.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error fetching station {station_id}: {e}\")\n",
    "\n",
    "# Merge with region_weather if data was found\n",
    "if best_patch is not None:\n",
    "    print(f\"\\nüöÄ Using patch from: {best_station}\")\n",
    "\n",
    "    region_weather_patched = pd.concat([region_weather, best_patch], ignore_index=True)\n",
    "    region_weather_patched.drop_duplicates(subset=['region', 'date'], keep='last', inplace=True)\n",
    "    region_weather_patched = region_weather_patched.sort_values(['region', 'date']).reset_index(drop=True)\n",
    "\n",
    "    region_weather_patched.to_csv(\n",
    "        \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Data/region_weather_epirus_patched.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Final patched dataset saved.\")\n",
    "else:\n",
    "    print(\"‚ùå No suitable fallback station had data for the requested period.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
