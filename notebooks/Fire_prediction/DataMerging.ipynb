{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook prepares the final modeling dataset by merging three critical data sources: historical daily weather observations, MODIS wildfire detection data, and administrative region boundaries for Greece. The process involves filtering and geolocating fire events, aggregating fire counts by region and day, and joining these with enriched weather data that includes terrain information. The resulting dataset links environmental conditions with fire occurrences over time and space, forming the foundation for training predictive model - XGboost_Predict_wildfire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "from shapely.geometry import Point\n",
    "from geopy.distance import geodesic\n",
    "from meteostat import Stations, Daily\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Data Collection and Regional Aggregation\n",
    "\n",
    "In this section, we collect and preprocess historical weather data for Greece from 2000 to 2024 using the Meteostat API. First, we fetch all weather stations in Greece and spatially join them to their corresponding administrative regions (from GADM boundaries). For each station, we retrieve daily weather variables such as temperature, precipitation, wind, and pressure. These are then aggregated by region and date to create a unified regional weather dataset. \n",
    "\n",
    "To ensure full coverage, we reindex the data to include all possible region-date combinations and fill missing entries by copying values from the geographically nearest region with available data on the same date. The final result is a complete, region-level weather time series ready for downstream modeling and fire prediction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GADM region boundaries for Greece\n",
    "greece_regions = gpd.read_file(\n",
    "    \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Data/gadm41_GRC_1.json\"\n",
    ")\n",
    "\n",
    "# Fetch Greek weather stations\n",
    "stations = Stations().region(\"GR\").fetch()\n",
    "stations_df = stations.reset_index()[['id', 'latitude', 'longitude']]\n",
    "\n",
    "# Convert stations to GeoDataFrame\n",
    "stations_gdf = gpd.GeoDataFrame(\n",
    "    stations_df,\n",
    "    geometry=gpd.points_from_xy(stations_df['longitude'], stations_df['latitude']),\n",
    "    crs=greece_regions.crs\n",
    ")\n",
    "\n",
    "# Spatially join stations to regions\n",
    "stations_with_regions = gpd.sjoin(\n",
    "    stations_gdf,\n",
    "    greece_regions[['NAME_1', 'geometry']],\n",
    "    predicate='intersects',\n",
    "    how='inner'\n",
    ").rename(columns={'NAME_1': 'region'})\n",
    "\n",
    "# Define time period\n",
    "start = datetime(2000, 1, 1)\n",
    "end = datetime(2024, 12, 31)\n",
    "\n",
    "weather_list = []\n",
    "weather_columns = ['tavg', 'tmin', 'tmax', 'prcp', 'snow', 'wdir', 'wspd', 'wpgt', 'pres', 'tsun']\n",
    "\n",
    "for station_id in stations_with_regions['id'].unique():\n",
    "    try:\n",
    "        weather = Daily(station_id, start, end).fetch()[weather_columns]\n",
    "        if not weather.empty:\n",
    "            weather['station_id'] = station_id\n",
    "            weather['date'] = weather.index\n",
    "            weather.reset_index(drop=True, inplace=True)\n",
    "            weather_list.append(weather)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching station {station_id}: {e}\")\n",
    "\n",
    "# Combine data\n",
    "weather_df = pd.concat(weather_list, ignore_index=True)\n",
    "\n",
    "# Merge with region data\n",
    "weather_with_regions = weather_df.merge(\n",
    "    stations_with_regions[['id', 'region']],\n",
    "    left_on='station_id',\n",
    "    right_on='id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Aggregate data by region and date\n",
    "region_weather = weather_with_regions.groupby(['region', 'date']).agg({\n",
    "    'tavg': 'mean',\n",
    "    'tmax': 'mean',\n",
    "    'tmin': 'mean',\n",
    "    'prcp': 'mean',\n",
    "    'snow': 'mean',\n",
    "    'wdir': 'mean',\n",
    "    'wspd': 'mean',\n",
    "    'wpgt': 'mean',\n",
    "    'pres': 'mean',\n",
    "    'tsun': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename to your preferred columns\n",
    "region_weather.rename(columns={\n",
    "    'tavg': 'temp_avg',\n",
    "    'tmax': 'temp_max',\n",
    "    'tmin': 'temp_min',\n",
    "    'prcp': 'precip',\n",
    "    'snow': 'snow_depth',\n",
    "    'wdir': 'wind_dir',\n",
    "    'wspd': 'wind_speed',\n",
    "    'wpgt': 'wind_gust',\n",
    "    'pres': 'pressure',\n",
    "    'tsun': 'sunshine_duration'\n",
    "}, inplace=True)\n",
    "\n",
    "# ----------- FILL MISSING REGION-DATE USING NEAREST REGION ----------- #\n",
    "\n",
    "# Generate full region-date index\n",
    "dates_full = pd.date_range(start=start, end=end)\n",
    "regions = region_weather['region'].unique()\n",
    "full_index = pd.MultiIndex.from_product([regions, dates_full], names=['region', 'date'])\n",
    "\n",
    "# Reindex to introduce missing rows\n",
    "region_weather_full = region_weather.set_index(['region', 'date']).reindex(full_index).reset_index()\n",
    "\n",
    "# Project to metric CRS for accurate centroids\n",
    "greece_regions['region'] = greece_regions['NAME_1']\n",
    "greece_projected = greece_regions.to_crs(epsg=3857)\n",
    "greece_regions['centroid'] = greece_projected['geometry'].centroid\n",
    "\n",
    "# Convert back to lat/lon for distance calcs\n",
    "greece_regions['centroid_coords'] = greece_regions['centroid'].to_crs(epsg=4326).apply(lambda g: (g.y, g.x))\n",
    "region_coords = greece_regions.set_index('region')['centroid_coords'].to_dict()\n",
    "\n",
    "# Fill missing rows using closest region with data on same date\n",
    "filled_rows = []\n",
    "for idx, row in region_weather_full[region_weather_full.isnull().any(axis=1)].iterrows():\n",
    "    target_region = row['region']\n",
    "    date = row['date']\n",
    "    target_coord = region_coords.get(target_region)\n",
    "    if not target_coord:\n",
    "        continue\n",
    "\n",
    "    available = region_weather_full[\n",
    "        (region_weather_full['date'] == date) &\n",
    "        (region_weather_full['region'] != target_region)\n",
    "    ].dropna()\n",
    "\n",
    "    if available.empty:\n",
    "        continue\n",
    "\n",
    "    available = available.copy()\n",
    "    available['dist'] = available['region'].apply(lambda r: geodesic(target_coord, region_coords[r]).km)\n",
    "    closest = available.sort_values('dist').iloc[0]\n",
    "    filled_row = row.copy()\n",
    "    filled_row.update(closest.drop(['region', 'date', 'dist']))\n",
    "    filled_rows.append(filled_row)\n",
    "\n",
    "# Merge back filled rows\n",
    "filled_df = pd.DataFrame(filled_rows)\n",
    "region_weather_final = pd.concat([\n",
    "    region_weather_full.dropna(),\n",
    "    filled_df\n",
    "], ignore_index=True).sort_values(['region', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Save final dataset\n",
    "region_weather_final.to_csv(\n",
    "    \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Data/region_weather_2000_2024.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(region_weather_final.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if all days had weather information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "region\n",
       "Aegean                          9132\n",
       "Crete                           9132\n",
       "Peloponnese,WesternGreeceand    9132\n",
       "MacedoniaandThrace              9126\n",
       "Attica                          9117\n",
       "ThessalyandCentralGreece        8882\n",
       "EpirusandWesternMacedonia       4129\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "region_weather['region'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this displays that i was not able to fetch every day from 3 of the regions. 9132, $(2024 - 2000 + 1) \\times 365 + \\text{leap days} = 25 \\times 365 + 7 = 9132$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch missing records, such they have weather information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fills in missing weather data for the Epirus and Western Macedonia region by checking several nearby fallback weather stations. It tries each station in turn and uses the first one that returns data for the period 2000‚Äì2011. Once valid data is found, it is cleaned and formatted to match the structure of the main dataset, then merged in to patch the missing entries. This step ensures that all regions, including Epirus, have complete weather records across the entire time range, which is important for consistency in model training and prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chatgpt prompt, 4o: \n",
    "\n",
    "I have a weather dataset for Greece aggregated by region and date, but one region (Epirus and Western Macedonia) has missing data for part of the time period (2000‚Äì2011). I want to fill the gap by using weather data from fallback stations located nearby.\n",
    "\n",
    "Please write Python code that:\n",
    "\t1.\tIterates through a list of fallback Meteostat station IDs\n",
    "\t2.\tFor each, tries to fetch weather data from 2000-01-01 to 2011-09-08\n",
    "\t3.\tIf data exists, clean and format it to match my main dataset‚Äôs schema (columns like temp_max, wind_speed, etc.)\n",
    "\t4.\tReplace any duplicate region-date pairs in the original data with the new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Trying station: Ioannina Airport (16642)\n",
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Kastoria Airport (16614)\n",
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Kozani Airport (16632)\n",
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Korca (13629)\n",
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Gjirokastra (13625)\n",
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Florina (16613)\n",
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Pretor-Pgc (13580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå No data available.\n",
      "\n",
      "üîç Trying station: Kerkyra Airport (16641)\n",
      "‚úÖ Data found: 4269 rows\n",
      "\n",
      "üöÄ Using patch from: Kerkyra Airport\n",
      "‚úÖ Final patched dataset saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of fallback station IDs near Epirus\n",
    "fallback_stations = [\n",
    "    ('16642', 'Ioannina Airport'),\n",
    "    ('16614', 'Kastoria Airport'),\n",
    "    ('16632', 'Kozani Airport'),\n",
    "    ('13629', 'Korca'),\n",
    "    ('13625', 'Gjirokastra'),\n",
    "    ('16613', 'Florina'),\n",
    "    ('13580', 'Pretor-Pgc'),\n",
    "    ('16641', 'Kerkyra Airport'),\n",
    "    ('16648', 'Larissa Airport'),\n",
    "    ('13583', 'Bitola')\n",
    "]\n",
    "\n",
    "# Define time range\n",
    "start = datetime(2000, 1, 1)\n",
    "end = datetime(2011, 9, 8)\n",
    "\n",
    "# Placeholder for best patch\n",
    "best_patch = None\n",
    "best_station = None\n",
    "\n",
    "# Try each fallback station\n",
    "for station_id, station_name in fallback_stations:\n",
    "    print(f\"\\nüîç Trying station: {station_name} ({station_id})\")\n",
    "    try:\n",
    "        patch_df = Daily(station_id, start, end).fetch()\n",
    "\n",
    "        if not patch_df.empty:\n",
    "            print(f\"Data found: {patch_df.shape[0]} rows\")\n",
    "\n",
    "            patch_df['date'] = patch_df.index\n",
    "            patch_df['region'] = 'EpirusandWesternMacedonia'\n",
    "            patch_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "            patch_df.rename(columns={\n",
    "                'tavg': 'temp_avg',\n",
    "                'tmax': 'temp_max',\n",
    "                'tmin': 'temp_min',\n",
    "                'prcp': 'precip',\n",
    "                'snow': 'snow_depth',\n",
    "                'wdir': 'wind_dir',\n",
    "                'wspd': 'wind_speed',\n",
    "                'wpgt': 'wind_gust',\n",
    "                'pres': 'pressure',\n",
    "                'tsun': 'sunshine_duration'\n",
    "            }, inplace=True)\n",
    "\n",
    "            patch_columns = ['region', 'date', 'temp_avg', 'temp_max', 'temp_min',\n",
    "                             'precip', 'snow_depth', 'wind_dir', 'wind_speed',\n",
    "                             'wind_gust', 'pressure', 'sunshine_duration']\n",
    "\n",
    "            patch_df = patch_df[patch_columns]\n",
    "\n",
    "            best_patch = patch_df\n",
    "            best_station = station_name\n",
    "            break  # Stop at first with data\n",
    "\n",
    "        else:\n",
    "            print(\"No data available.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching station {station_id}: {e}\")\n",
    "\n",
    "# Merge with region_weather if data was found\n",
    "if best_patch is not None:\n",
    "    print(f\"\\nüöÄ Using patch from: {best_station}\")\n",
    "\n",
    "    region_weather_patched = pd.concat([region_weather, best_patch], ignore_index=True)\n",
    "    region_weather_patched.drop_duplicates(subset=['region', 'date'], keep='last', inplace=True)\n",
    "    region_weather_patched = region_weather_patched.sort_values(['region', 'date']).reset_index(drop=True)\n",
    "\n",
    "    region_weather_patched.to_csv(\n",
    "        \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Data/region_weather_epirus_patched.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "    print(\"Final patched dataset saved.\")\n",
    "else:\n",
    "    print(\"No suitable fallback station had data for the requested period.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining weather and fire events with elevation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chatgpt 4o, prompt: \n",
    "\n",
    "Write a full Python script that enriches a weather and wildfire event dataset by adding average terrain elevation for each region. The script should: (1) load a CSV file containing weather features, wildfire occurrence labels, and a ‚Äòregion‚Äô column; (2) load a GeoJSON file containing geographic boundaries (polygons) for each region; (3) for each region, randomly sample approximately 150 latitude-longitude points inside the region‚Äôs polygon; (4) fetch elevation data for each sampled point using the Open-Elevation API (https://api.open-elevation.com/api/v1/lookup); (5) calculate the average elevation for each region; (6) merge the average elevation back into the original weather dataset based on the region; and (7) save the final enriched dataset to a new CSV file. The script should include retries for API calls, pause politely between requests (e.g., 0.2 seconds), print progress updates, and handle errors gracefully"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scripts adds terrain data to each record based on its region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Sampling points and fetching elevations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [08:30<00:00, 63.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building region elevation dataframe...\n",
      "Merging elevation into weather dataset...\n",
      "Saved enriched dataset to: /Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Wildfire Prediction/data/raw/fire_events_with_weather_with_elevation.csv\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIG ---\n",
    "# Paths\n",
    "weather_file = \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Wildfire Prediction/data/raw/fire_events_with_weather.csv\"\n",
    "gadm_file = \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Wildfire Prediction/data/raw/gadm41_GRC_1.json\"\n",
    "output_file = \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Wildfire Prediction/data/raw/fire_events_with_weather_with_elevation.csv\"\n",
    "\n",
    "# Sampling settings\n",
    "samples_per_region = 300  # Number of random points to sample per region\n",
    "\n",
    "# API settings\n",
    "api_url = \"https://api.open-elevation.com/api/v1/lookup\"\n",
    "pause_between_calls = 0.2  # seconds\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "print(\"Loading datasets...\")\n",
    "weather_df = pd.read_csv(weather_file)\n",
    "gadm = gpd.read_file(gadm_file)\n",
    "gadm = gadm.rename(columns={'NAME_1': 'region'})\n",
    "\n",
    "# --- FUNCTION: Fetch elevation ---\n",
    "def fetch_elevation(lat, lon, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(f\"{api_url}?locations={lat},{lon}\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                result = response.json().get('results', [{}])[0]\n",
    "                elevation = result.get('elevation')\n",
    "                if elevation is not None:\n",
    "                    return elevation\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(pause_between_calls)\n",
    "    return None\n",
    "\n",
    "# --- FUNCTION: Sample points inside polygon ---\n",
    "def sample_points_in_polygon(polygon, num_points):\n",
    "    points = []\n",
    "    minx, miny, maxx, maxy = polygon.bounds\n",
    "    while len(points) < num_points:\n",
    "        random_point = Point(np.random.uniform(minx, maxx), np.random.uniform(miny, maxy))\n",
    "        if polygon.contains(random_point):\n",
    "            points.append((random_point.y, random_point.x))  # lat, lon\n",
    "    return points\n",
    "\n",
    "# --- MAIN: Calculate average, max, and min elevation per region ---\n",
    "print(\"Sampling points and fetching elevations...\")\n",
    "region_elevations = []\n",
    "\n",
    "for idx, row in tqdm(gadm.iterrows(), total=gadm.shape[0]):\n",
    "    region_name = row['region']\n",
    "    polygon = row['geometry']\n",
    "\n",
    "    # Sample points\n",
    "    points = sample_points_in_polygon(polygon, samples_per_region)\n",
    "\n",
    "    # Fetch elevations\n",
    "    elevations = []\n",
    "    for lat, lon in points:\n",
    "        elev = fetch_elevation(lat, lon)\n",
    "        if elev is not None:\n",
    "            elevations.append(elev)\n",
    "\n",
    "    if elevations:\n",
    "        avg_elevation = np.mean(elevations)\n",
    "        max_elevation = np.max(elevations)\n",
    "        min_elevation = np.min(elevations)\n",
    "    else:\n",
    "        avg_elevation = np.nan\n",
    "        max_elevation = np.nan\n",
    "        min_elevation = np.nan\n",
    "\n",
    "    region_elevations.append((region_name, avg_elevation, max_elevation, min_elevation))\n",
    "\n",
    "# --- BUILD DATAFRAME ---\n",
    "print(\"Building region elevation dataframe...\")\n",
    "region_elevation_df = pd.DataFrame(region_elevations, columns=['region', 'avg_elevation', 'max_elevation', 'min_elevation'])\n",
    "\n",
    "# --- MERGE into weather dataset ---\n",
    "print(\"Merging elevation into weather dataset...\")\n",
    "weather_df = weather_df.merge(region_elevation_df, on='region', how='left')\n",
    "\n",
    "# --- SAVE ---\n",
    "weather_df.to_csv(output_file, index=False)\n",
    "print(f\"Saved enriched dataset to: {output_file}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Wildfire Prediction/data/raw/fire_events_with_weather_with_elevation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "region",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "date",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "temp_avg",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_max",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temp_min",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "snow_depth",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_dir",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_speed",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_gust",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "pressure",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sunshine_duration",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wildfire_count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "wildfire_occurred",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "avg_elevation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "max_elevation",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "min_elevation",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "66024379-9986-41ac-aaa8-439c5c9f20a8",
       "rows": [
        [
         "0",
         "Aegean",
         "2000-01-01",
         "12.325",
         "14.85",
         "10.9",
         "7.1",
         null,
         "309.0",
         "17.333333333333332",
         null,
         null,
         null,
         "0",
         "0",
         "201.19",
         "988.0",
         "0.0"
        ],
        [
         "1",
         "Aegean",
         "2000-01-02",
         "10.475",
         "11.7",
         "8.8",
         "0.0",
         null,
         "192.5",
         "16.566666666666666",
         null,
         null,
         null,
         "0",
         "0",
         "201.19",
         "988.0",
         "0.0"
        ],
        [
         "2",
         "Aegean",
         "2000-01-03",
         "7.066666666666666",
         "8.733333333333333",
         "4.933333333333334",
         "1.3",
         null,
         "184.5",
         "33.95",
         null,
         null,
         null,
         "0",
         "0",
         "201.19",
         "988.0",
         "0.0"
        ],
        [
         "3",
         "Aegean",
         "2000-01-04",
         "6.35",
         "7.5",
         "5.25",
         "0.0",
         null,
         "330.0",
         "36.2",
         null,
         null,
         null,
         "0",
         "0",
         "201.19",
         "988.0",
         "0.0"
        ],
        [
         "4",
         "Aegean",
         "2000-01-05",
         "6.5",
         "8.533333333333333",
         "3.466666666666667",
         "0.0",
         null,
         "317.0",
         "21.05",
         null,
         null,
         null,
         "0",
         "0",
         "201.19",
         "988.0",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 17,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>date</th>\n",
       "      <th>temp_avg</th>\n",
       "      <th>temp_max</th>\n",
       "      <th>temp_min</th>\n",
       "      <th>precip</th>\n",
       "      <th>snow_depth</th>\n",
       "      <th>wind_dir</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>wind_gust</th>\n",
       "      <th>pressure</th>\n",
       "      <th>sunshine_duration</th>\n",
       "      <th>wildfire_count</th>\n",
       "      <th>wildfire_occurred</th>\n",
       "      <th>avg_elevation</th>\n",
       "      <th>max_elevation</th>\n",
       "      <th>min_elevation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aegean</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>12.325000</td>\n",
       "      <td>14.850000</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>7.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>309.0</td>\n",
       "      <td>17.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>201.19</td>\n",
       "      <td>988.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aegean</td>\n",
       "      <td>2000-01-02</td>\n",
       "      <td>10.475000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>192.5</td>\n",
       "      <td>16.566667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>201.19</td>\n",
       "      <td>988.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aegean</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>7.066667</td>\n",
       "      <td>8.733333</td>\n",
       "      <td>4.933333</td>\n",
       "      <td>1.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>184.5</td>\n",
       "      <td>33.950000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>201.19</td>\n",
       "      <td>988.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aegean</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>6.350000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>5.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>330.0</td>\n",
       "      <td>36.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>201.19</td>\n",
       "      <td>988.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aegean</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>8.533333</td>\n",
       "      <td>3.466667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>317.0</td>\n",
       "      <td>21.050000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>201.19</td>\n",
       "      <td>988.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   region        date   temp_avg   temp_max   temp_min  precip  snow_depth  \\\n",
       "0  Aegean  2000-01-01  12.325000  14.850000  10.900000     7.1         NaN   \n",
       "1  Aegean  2000-01-02  10.475000  11.700000   8.800000     0.0         NaN   \n",
       "2  Aegean  2000-01-03   7.066667   8.733333   4.933333     1.3         NaN   \n",
       "3  Aegean  2000-01-04   6.350000   7.500000   5.250000     0.0         NaN   \n",
       "4  Aegean  2000-01-05   6.500000   8.533333   3.466667     0.0         NaN   \n",
       "\n",
       "   wind_dir  wind_speed  wind_gust  pressure  sunshine_duration  \\\n",
       "0     309.0   17.333333        NaN       NaN                NaN   \n",
       "1     192.5   16.566667        NaN       NaN                NaN   \n",
       "2     184.5   33.950000        NaN       NaN                NaN   \n",
       "3     330.0   36.200000        NaN       NaN                NaN   \n",
       "4     317.0   21.050000        NaN       NaN                NaN   \n",
       "\n",
       "   wildfire_count  wildfire_occurred  avg_elevation  max_elevation  \\\n",
       "0               0                  0         201.19          988.0   \n",
       "1               0                  0         201.19          988.0   \n",
       "2               0                  0         201.19          988.0   \n",
       "3               0                  0         201.19          988.0   \n",
       "4               0                  0         201.19          988.0   \n",
       "\n",
       "   min_elevation  \n",
       "0            0.0  \n",
       "1            0.0  \n",
       "2            0.0  \n",
       "3            0.0  \n",
       "4            0.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             region  avg_elevation  max_elevation  \\\n",
      "27381     EpirusandWesternMacedonia     841.143333         2200.0   \n",
      "44905  Peloponnese,WesternGreeceand     489.623333         1965.0   \n",
      "54037      ThessalyandCentralGreece     484.766667         1979.0   \n",
      "18249                         Crete     470.780000         2148.0   \n",
      "35779            MacedoniaandThrace     382.493333         2205.0   \n",
      "9132                         Attica     261.163333         1071.0   \n",
      "0                            Aegean     201.190000          988.0   \n",
      "\n",
      "       min_elevation  \n",
      "27381           -5.0  \n",
      "44905           -2.0  \n",
      "54037            0.0  \n",
      "18249           -2.0  \n",
      "35779           -3.0  \n",
      "9132             0.0  \n",
      "0                0.0  \n"
     ]
    }
   ],
   "source": [
    "region_elevation = df[['region', 'avg_elevation','max_elevation', 'min_elevation']].drop_duplicates()\n",
    "\n",
    "# Sort for better viewing\n",
    "region_elevation = region_elevation.sort_values('avg_elevation', ascending=False)\n",
    "\n",
    "print(region_elevation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code merges satellite-based fire occurrence data from the MODIS archive with an enriched regional weather dataset we added elevation data to. It first filters the fire data to include only high-confidence detections (confidence ‚â• 30), then performs a spatial join to map each fire point to its corresponding Greek administrative region. After assigning fires to regions and dates, it aggregates fire counts and adds two key columns to the weather dataset:\n",
    "\n",
    "\t‚Ä¢\twildfire_occurred: a binary indicator (1 if any fire occurred that day in the region)\n",
    "\t‚Ä¢\twildfire_count: the total number of fires detected in that region on that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Filtering MODIS fires by confidence ‚â• 30...\n",
      "Assigning fires to regions...\n",
      "Aggregating fire counts...\n",
      "Merging fire data with weather + terrain data...\n",
      "Final enriched dataset saved to: /Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Wildfire Prediction/data/raw/final_fire_weather_with_elevation.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- CONFIG ---\n",
    "weather_file = \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Wildfire Prediction/data/raw/fire_events_with_weather_with_elevation.csv\"\n",
    "modis_file = \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Wildfire Prediction/data/raw/fire_archive_M-C61_586436.csv\"\n",
    "gadm_file = \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Wildfire Prediction/data/raw/gadm41_GRC_1.json\"\n",
    "output_file = \"/Users/Thomas/Desktop/Skole/Business Analytics/Advanced Business Analytics/Wildfire Prediction/data/raw/final_fire_weather_with_elevation.csv\"\n",
    "\n",
    "# --- LOAD DATA ---\n",
    "print(\"Loading datasets...\")\n",
    "weather_df = pd.read_csv(weather_file)\n",
    "weather_df['date'] = pd.to_datetime(weather_df['date'])\n",
    "\n",
    "modis_df = pd.read_csv(modis_file)\n",
    "modis_df['acq_date'] = pd.to_datetime(modis_df['acq_date'])\n",
    "\n",
    "gadm = gpd.read_file(gadm_file)\n",
    "gadm = gadm.rename(columns={'NAME_1': 'region'})\n",
    "\n",
    "# --- FILTER MODIS FIRES BY CONFIDENCE ---\n",
    "print(\"Filtering MODIS fires by confidence ‚â• 30...\")\n",
    "modis_filtered = modis_df[modis_df['confidence'] >= 30]\n",
    "\n",
    "# --- Spatial join: Assign MODIS fires to regions ---\n",
    "print(\"Assigning fires to regions...\")\n",
    "modis_gdf = gpd.GeoDataFrame(\n",
    "    modis_filtered,\n",
    "    geometry=gpd.points_from_xy(modis_filtered['longitude'], modis_filtered['latitude']),\n",
    "    crs=gadm.crs\n",
    ")\n",
    "\n",
    "modis_with_region = gpd.sjoin(\n",
    "    modis_gdf,\n",
    "    gadm[['region', 'geometry']],\n",
    "    how='inner',\n",
    "    predicate='intersects'\n",
    ").drop(columns=['index_right'])\n",
    "\n",
    "# --- Standardize column names ---\n",
    "modis_with_region.rename(columns={'acq_date': 'date'}, inplace=True)\n",
    "modis_with_region['date'] = pd.to_datetime(modis_with_region['date'])\n",
    "\n",
    "# --- Aggregate fires per region and date ---\n",
    "print(\"Aggregating fire counts...\")\n",
    "fire_counts = modis_with_region.groupby(['region', 'date']).size().reset_index(name='wildfire_count')\n",
    "fire_counts['wildfire_occurred'] = 1\n",
    "\n",
    "# --- Merge fire counts into enriched weather dataset ---\n",
    "print(\"Merging fire data with weather + terrain data...\")\n",
    "weather_labeled = weather_df.merge(fire_counts, on=['region', 'date'], how='left')\n",
    "\n",
    "# --- Ensure wildfire_occurred and wildfire_count columns exist and are safe ---\n",
    "for col in ['wildfire_occurred', 'wildfire_count']:\n",
    "    if col not in weather_labeled.columns:\n",
    "        weather_labeled[col] = 0\n",
    "    else:\n",
    "        weather_labeled[col] = weather_labeled[col].fillna(0).astype(int)\n",
    "\n",
    "# --- SAVE FINAL DATASET ---\n",
    "weather_labeled.to_csv(output_file, index=False)\n",
    "print(f\"Final enriched dataset saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a final note now we have the final dataset \"Wildfire Prediction/data/processed/fire_events_with_weather_with_elevation.csv\" that is going to be used to predict wildfire in greece based on historical weather, elevation and historical wildfires "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
